\section{Convolution Models}
\todo{Here we describe all the stuff that is common to both modeling and tracking applications: correspondence computation and rendering.}

\input{fig/posing/item.tex}
\subsection{Posing the model}
\label{sec:posing}
% 
\begin{DRAFT}
To pose a convolution model the centers ... 
Where $\Pi$ left multiplies matrices by traversing the kinematic chain $K(i)$ of element $i$ towards the root.
... $\bar\ballcenter_i$ are written in world coordinates.
% 
\begin{equation}
\ballcenter_i = \left[ \Pi_{k \in K(i)} \mathbf{\bar{T}}_k \mathbf{T}_k \mathbf{\bar{T}}_k^{-1} \right] \bar\ballcenter_i
\label{eq:kinematic}
\end{equation}
% 
Basically we express the point int he coordinate frame of the k-th element, we apply the posing transformation, then we re-apply the rest pose transformation to bring the point back in world coordinates. ... \TODO{mention \Figure{posing}}
\end{DRAFT}

\subsection{Correspondences}
\label{sec:corresp}
% \paragraph{Surface Correspondences: $\proj_\surface(\point)$}
Our correspondence search leverages the structure of \Eq{convsurf}, by decomposing the surface in several elementary convolution elements $\element$ (a decomposition of the shape in \todo{pill} and \todo{wedge} implicit primitives) and their associated implicit functions $\implicit_e$. Given a point $\point$ in space, the implicit function of the whole surface can be written by evaluating the expression:
\begin{equation}
\implicit_\surface(\point) = \argmin_{e=1 \dots E} \implicit_e(\point)
\label{eq:piecewise}
\end{equation}
Therefore, given a query point $\point$, we can first compute the closest-points $\footpoint_e = \proj_{\element}(\point)$ to each element independently; within this set, the closest-point projection to the full model $\footpoint = \proj_\surface(\point)$ is the one with the smallest associated implicit function value $\implicit_e(\point)$. In a tracking session with an average of \todo{1000 points/frame} the computation of closest-point correspondences takes \todo{300 $\mu s$/iteration}. We now describe in details how the projection is evaluated on each element in \todo{close form}. 

\input{fig/corresp/item.tex}
\paragraph{Pill Correspondences: $\footpoint=\proj_\text{pill}(\point)$}
A pill is defined by two balls $\ball_1(\ballcenter_1,r_1)$ and $\ball_2(\ballcenter_2,r_2)$. 
\begin{DRAFT}
Let us first compute the position of the skewed projection $\skewproj$; see \Figure{corresp}-(left). 
% 
\AT{re-write using the simplified approach}
% 
Let us assume $r_1 > r_2$ and define $\mathbf{u} = \ballcenter_2-\ballcenter_1$, $\mathbf{\vec{u}} = \mathbf{u}/\|\mathbf{u}\|$, $\mathbf{v} = \point-\ballcenter_1$, $\alpha=\mathbf{\vec{u}} \cdot \mathbf{v}$. The orthogonal projection of $\point$ onto the segment is $\mathbf{t} = \mathbf{c}_1 + \alpha \mathbf{u}$. The distance $\omega$ can be derived with the Pythagorean theorem as $\omega^2 = \mathbf{u}^T \mathbf{u} - (r_1 - r_2)^2$ while $\delta$ is derived leveraging triangle similarity $\delta / \| \point - \mathbf{t} \| = (r_1-r_2)/\omega$. Finally, our skewed projection $\skewproj$ is $\skewproj = \mathbf{t} - \delta \mathbf{\vec{u}}$.
% 
According to the position of $\skewproj$ we have two solutions: 
(1) If $\skewproj$ lies on the segment $\segment$, then $\point$ projects onto the pill's conic surface as $\footpoint=\mathbf{s}+(\gamma+r_2) \overrightarrow{\mathbf{\skewproj} \point}$, where $\overrightarrow{\skewproj \point} = (\point-\skewproj)/ \|\point-\skewproj\|$. 
(2) if $\skewproj$ lies outside the segment and closer to the ball $\ball_*$, then $\point$ projects onto the sphere as $\footpoint_e = \ballcenter_* + r_*(\overrightarrow{\point \ballcenter_*})$.
\end{DRAFT}

\paragraph{Wedge Correspondences: $\footpoint=\proj_\text{wedge}(\point)$}
A wedge is defined by three spheres $\ball_i = \{\ballcenter_i, r_i\}$. By inspecting \Figure{convsurf}, it is apparent that a wedge element can be decomposed in three parts: \emph{spherical}, \emph{conical} and \emph{planar} elements respectively associated with vertices, edges and faces of the triangular convolution skeleton. Having identified the planar element $\mathcal{P}(\mathbf{n}, \mathbf{t}_0)$ with normal $\mathbf{n}$ and tangent $\mathbf{t}_0$ to $\ball_0$ (see \Appendix{wedgecorr}), we compute the skewed projection $\skewproj$ by finding the intersection of the ray $\mathbf{r}(t) = \point + t\mathbf{n}$ with the triangle $\mathcal{T}$ formed by $\ballcenter_1$, $\ballcenter_2$ and $\ballcenter_3$. 
% 
According to the position of $\skewproj$ we have two solutions:
(1) If the skewed $\skewproj$ lies inside the triangle $\mathcal{T}$, then our footpoint is $\footpoint = \proj_\mathcal{P}(\point)$, otherwise 
(2) employ the barycentric coordinates of $\skewproj$ in $\mathcal{T}$ to identify the closest pill element and compute $\footpoint=\proj_\text{pill}(\point)$.

\input{fig/calibration/item.tex}
\subsection{Rendering}
Rendering the convolution model in real time is not only employed for visual verification of tracking performance; e.g. \Figure{coarsemodel}. The real-time tracking algorithm reviewed in \Section{overview} performs a 2D registration in the image plane that requires the computation of an (image-space) silhouette. There are two alternatives for rendering a  convolution model like the one shown in \Figure{topology}. One possibility is to explicitly extract the surface of individual convolution element by computing the convex hull of pairs or triplets of spheres; see \Figure{convsurf} and \cite{ando2013liquid}. While this process would be suitable in applications where the model is fixed, it is hardly appropriate to our scenario where we want to calibrate the model to the user. Therefore, we instead efficiently ray-trace the convolution model on the GPU. We render a unit fullscreen quad, and in the fragment shader we leverage the camera intrinsics to compute the camera ray $\mathbf{r}(\mathbf{x})$ associated with each pixel $\mathbf{x}$. Each of this ray is intersected with each convolution element of our model in \todo{close form}, and only the closest intersection point is retained. \AT{how do you perform the intersection tests? do you decompose the surfaces into planes, cones, and spheres or what?} Surface normals at the intersection point are also computed for shading purposes.

\paragraph{Optimization}
Ray tracing our surface on a framebuffer of size $1280 \times 960$ would consume in average \todo{$\sim 4$ ms/iteration}. However, our optimization requires in the order of 10 iterations to converge, thus upper-bounding the performance of the tracker to 25 \FPS{}. We overcome this issue by rendering the model to a smaller $320 \times 240$ framebuffer texture (the same resolution of textures fetched from the sensor) and upsampling it with linearly interpolation whenever necessary. This process allows us to reduce the rendering time to~\todo{$\sim.6$~ms/iteration}.

\endinput

% It is required to render the model for silhouette energy and for visualization purposes. The full rendering process is done in fragment shader, the vertex shader only provides a quad.
% In fragment shader, given the fragment coordinates gl\_FragCoord, we compute the camera ray corresponding to the current pixel, but de-applying the transformations done by the rasterizer to the points $\{$gl\_FragCoord.x, gl\_FragCoord.y, 0$\}$ and $\{$gl\_FragCoord.x, gl\_FragCoord.y, 1$\}$ that correspond to the points on the near and far camera plane in the world coordinates.
% Once we have the ray, we intersect it with each block of the model. The intersection points of a ray with a sphere, plane and conic surface are found in closed form.
% \textbf{Optimization.}
% We also compute the normals at intersection points for shading. This takes 20 milliseconds for a window size $1280 \times 960$ which is prohibitively show. To speed up the rendering, we first render an indicator texture of size $320 \times 240$ that contains model silhouette with the number of each model block and a value 255 for background. Once the texture is accessed from the fragment shader, it is automatically interpolated for the window size  $1280 \times 960$.