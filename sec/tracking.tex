\section{Tracking}

\input{fig/corresp/item.tex}

\input{sec/element_correspondences}

\paragraph{Element-Wise Correspondences}
Our correspondence search leverages the structure of \Eq{convsurf}, by decomposing the surface in several elementary convolution elements $\element$ (a decomposition of the shape in \todo{pill} and \todo{wedge} implicit primitives) and their associated implicit functions $\implicit_e$. Given a point $\point$ in space, the implicit function of the whole surface can be written by evaluating the expression:
\begin{equation}
\implicit_\surface(\point) = \argmin_{e=1 \dots E} \implicit_e(\point)
\label{eq:piecewise}
\end{equation}
Therefore, given a query point $\point$, we can first compute the closest-points $\footpoint_e = \proj_{\element}(\point)$ to each element independently; within this set, the closest-point projection to the full model $\footpoint = \proj_\surface(\point)$ is the one with the smallest associated implicit function value $\implicit_e(\point)$. In a tracking session with an average of \todo{1000 points/frame} the computation of closest-point correspondences takes \todo{300 $\mu s$/iteration}.

\paragraph{Frontal Correspondences}
In monocular acquisition, an oracle registration algorithm aligns the portion of the model that is \emph{visible} from sensor viewpoint to the available data. Hence, when computing ICP's closest-point correspondences, only the portion of the model currently visible by the camera should be considered~\cite{tagliasacchi2015robust}. Given the camera direction $\camdir$, we can test whether the retrieved footpoint $\footpoint$ is back-facing by testing the sign of $\camdir \cdot \mathcal{N}_\surface(\footpoint)$, where the second term is the object's normal at $\footpoint$. As illustrated in 2D in \Figure{visibility}, whenever this test fails, there are additional candidates for closest point that must be checked: (1)~the closest-point on the silhouette of the model (e.g. $\point_{2,4,5,7}$), and (2)~the front facing planar portions of convolution elements (e.g. $\point_{3}$). These additional correspondences for the query point are computed, and the one closest to $\point$ becomes our front-facing footpoint $\footpoint$. The additional computational cost caused by front-facing correspondences with an average of \todo{1000 points/frame} is \todo{100 $\mu s$/iteration}.
\input{fig/visibility/item.tex}

\input{fig/silhouette/item.tex}
\paragraph{Silhouette Computation}
The \emph{object-space silhouette} $\partial \surface$ is a (3D) curve separating front-facing from back-facing portions of a shape~\cite[Sec.1]{olson2006eg}. Similarly to \cite{tagliasacchi2015robust}, to simplify the silhouette computation we approximate the perspective camera of the sensor with an orthographic one. We then offset all convolution elements on the 2D camera plane, and perform a cross-section with this plane. Spheres are replaced with circles and planes/cylinders with segments; see~\Figure{silhouette}-(left). We then compute an \emph{arrangement}, splitting curves whenever intersection or tangency occurs~\Figure{silhouette}-(center). We traverse this graph, starting from a point that is guaranteed to be on the outline (e.g. a point on the bounding box). The traversal selects the next element as the one whose tangent forms the smallest counter-clockwise angle thus identifying the silhouette. Once the 2D silhouette has been computed, it can be re-projected in 3D; see \Figure{silhouette}. Note the process described above would compute the \emph{image-space silhouette} of our model. Therefore, we apply the process above to palm and fingers separately, and merge them in a second phase. The merge process simply checks whether vertices $v \in \partial \surface$ are contained within the model, that is it discards those where $\implicit_\surface(v)<0$. In our experiments, the average computation of silhouette on the CPU takes \todo{\emph{100 $\mu$s/iteration}}.