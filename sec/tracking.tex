\section{Tracking}

\input{fig/corresp/item.tex}
\paragraph{Element Correspondences}
\TODO{Anastasia, please add and describe \Figure{corresp}, that is how do we compute correspondences to a single convolution element}.

\paragraph{Element-Wise Correspondences}
Our correspondence search leverages the structure of \Eq{convsurf}, by decomposing the surface in several elementary convolution elements $\element$ (a decomposition of the shape in \todo{pill} and \todo{wedge} implicit primitives) and their associated implicit functions $\implicit_e$. Given a point $\point$ in space, the implicit function of the whole surface can be written by evaluating the expression:
\begin{equation}
\implicit_\surface(\point) = \argmin_{e=1 \dots E} \implicit_e(\point)
\label{eq:piecewise}
\end{equation}
Therefore, given a query point $\point$, we can first compute the closest-points $\footpoint_e = \proj_{\element}(\point)$ to each element independently; within this set, the closest-point projection to the full model $\footpoint = \proj_\surface(\point)$ is the one with the smallest associated implicit function value $\implicit_e(\point)$.
\TODO{should we mention how long this takes?}

\paragraph{Frontal Correspondences}
In monocular acquisition, an oracle registration algorithm aligns the portion of the model that is \emph{visible} from sensor viewpoint to the available data. Hence, when computing ICP's closest-point correspondences, only the portion of the model currently visible by the camera should be considered~\cite{tagliasacchi2015robust}. Given the camera direction $\camdir$, we can test whether the retrieved footpoint $\footpoint$ is back-facing by testing the sign of $\camdir \cdot \mathcal{N}_\surface(\footpoint)$, where the second term is the object's normal at $\footpoint$. As illustrated in 2D in \Figure{visibility}, whenever this test fails, there are additional candidates for closest point that must be checked: (1)~the closest-point on the silhouette of the model (e.g. $\point_{2,4,5,7}$), and (2)~the front facing planar portions of convolution elements (e.g. $\point_{3}$). These additional correspondences for the query point are computed, and the one closest to $\point$ becomes our front-facing footpoint $\footpoint$.
\input{fig/visibility/item.tex}


\input{fig/silhouette/item.tex}
\paragraph{Silhouette Computation}
The \emph{silhouette} is a (3D) curve separating front-facing from back-facing portions of a shape. In our visibility-aware search, many points will be mapped to this curve. Similarly to \cite{tagliasacchi2015robust}, to compute the silhouette we approximate the perspective camera of the sensor with an orthographic one. Under this assumption finding the (3D) silhouette $\partial \surface$ becomes much simpler. We first project the 3D convolution model onto the 2D camera plane, and then traverse the graph of line and circle segments in counters-clockwise direction. \AT{this is not very clear to me, needs to be explained better} Once the 2D silhouette has been computed, it can be re-projected in 3D; see \Figure{silhouette}.

\AT{do you have to pass the silhouette back to CUDA to compute correspondences?}

\FINISH
