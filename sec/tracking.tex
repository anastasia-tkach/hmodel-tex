\section{Tracking}
\begin{DRAFT}
High level description of these three components:
(1) We need to modify correspondences to make them compatible to single vantage point, 
(2) we need to render the convolution surface to compute 2D alignment energies, 
(3)
\end{DRAFT}


\subsection{Correspondences}
% \paragraph{Frontal Correspondences}
In monocular acquisition, an oracle registration algorithm aligns the portion of the model that is \emph{visible} from sensor viewpoint to the available data. Hence, when computing ICP's closest-point correspondences, only the portion of the model currently visible by the camera should be considered~\cite{tagliasacchi2015robust}. Given the camera direction $\camdir$, we can test whether the retrieved footpoint $\footpoint$ is back-facing by testing the sign of $\camdir \cdot \mathcal{N}_\surface(\footpoint)$, where the second term is the object's normal at $\footpoint$. As illustrated in 2D in \Figure{visibility}, whenever this test fails, there are additional candidates for closest point that must be checked: (1)~the closest-point on the silhouette of the model (e.g. $\point_{2,4,5,7}$), and (2)~the front facing planar portions of convolution elements (e.g. $\point_{3}$). These additional correspondences for the query point are computed, and the one closest to $\point$ becomes our front-facing footpoint $\footpoint$. The additional computational cost caused by front-facing correspondences with an average of \todo{1000 points/frame} is \todo{100 $\mu s$/iteration}.
\input{fig/visibility/item.tex}

\input{fig/silhouette/item.tex}
\paragraph{Silhouette Computation}
The \emph{object-space silhouette} $\partial \surface$ is a (3D) curve separating front-facing from back-facing portions of a shape~\cite[Sec.1]{olson2006eg}. Similarly to \cite{tagliasacchi2015robust}, to simplify the silhouette computation we approximate the perspective camera of the sensor with an orthographic one. We then offset all convolution elements on the 2D camera plane, and perform a cross-section with this plane. Spheres are replaced with circles and planes/cylinders with segments; see~\Figure{silhouette}-(left). We then compute an \emph{arrangement}, splitting curves whenever intersection or tangency occurs~\Figure{silhouette}-(center). We traverse this graph, starting from a point that is guaranteed to be on the outline (e.g. a point on the bounding box). The traversal selects the next element as the one whose tangent forms the smallest counter-clockwise angle thus identifying the silhouette. Once the 2D silhouette has been computed, it can be re-projected in 3D; see \Figure{silhouette}. Note the process described above would compute the \emph{image-space silhouette} of our model. Therefore, we apply the process above to palm and fingers separately, and merge them in a second phase. The merge process simply checks whether vertices $v \in \partial \surface$ are contained within the model, that is it discards those where $\implicit_\surface(v)<0$. In our experiments, the average computation of silhouette on the CPU takes \todo{\emph{100 $\mu$s/iteration}}.

\subsection{Rendering}

\begin{DRAFT}
It is required to render the model for silhouette energy and for visualization purposes. The full rendering process is done in fragment shader, the vertex shader only provides a quad. 
In fragment shader, given the fragment coordinates gl\_FragCoord, we compute the camera ray corresponding to the current pixel, but de-applying the transformations done by the rasterizer to the points $\{$gl\_FragCoord.x, gl\_FragCoord.y, 0$\}$ and $\{$gl\_FragCoord.x, gl\_FragCoord.y, 1$\}$ that correspond to the points on the near and far camera plane in the world coordinates.

Once we have the ray, we intersect it with each block of the model. The intersection points of a ray with a sphere, plane and conic surface are found in closed form. We also compute the normals at intersection points for shading. This takes 20 milliseconds for a window size $1280 \times 960$ which is prohibitively show.

\textbf{Optimization.} To speed up the rendering, we first render an indicator texture of size $320 \times 240$ that contains model silhouette with the number of each model block and a value 255 for background. Once the texture is accessed from the fragment shader, it is automatically interpolated for the window size  $1280 \times 960$. We only consider the rays for which the value of the indicator texture is less than 255. The change in resolution is handled by the ``blured'' boundaries of the model. This allows to decrease rendering time to 6-7 millisecond. 

We use one more heuristic, if the value of the indicator texture is very close to an integer, this means that the pixel is on the middle of a block and we can only intersect the corresponding ray with that block. If the  value is not an integer, than the intersection is done with all the blocks. This brings the rendering time down to 4-5 ms. \Anastasia{But with this I get white pixels at the intersection line of the blocks that have very different indices, like 3 and 20. I have no idea why. Also, if while reading this you got an idea how to speed up rendering some more, please let me know, it would be great. Also I tried running glFinish, after offscreen renderer and nothing changed, I still have exactly the same issues.}
\end{DRAFT}

\subsection{Initial transformations}
\begin{DRAFT}
The overwhelming majority of hand  tracking systems uses joint angles as an optimization variable. Given the vector of joint angles $\theta$, one also needs to know the initial rotations of the fingers with respect to the palm to be able to pose the model. (This is especially relevant for user-specific model adaptation). The precise tracking is impossible without having the correct initial rotations, because the joints at the base of fingers cannot rotate around their axis, thus initial rotations constrain the possible poses that a hand model can assume. The initial rotations of the fingers can be guessed approximately because the are close to identity, but with the thumb it is much more difficult. \Anastasia{To prove this, I could alter the rotations in our system and show that tracking gets worse.} 
To compute initial rotations we formulate an optimization problem for all the input hand poses. Denote the set of model centers in pose $i$ as $C_i = \{c_1^i, ... c_{|C_i| }^i\}$. As a preprocessing step we compute a rigid rotation between the rigid palm centers (the centers that are not articulated or elastic) and rotate all the poses to the same position, such that base centers of their fingers match together.

The initial rotation is computed separately for each finger, so let us just discuss one finger. We minimize the norm of the difference between the centers of the finger at the current $i$ pose $\{c_1^i, c_2^i, c_3^i \}$ and the positions of the centers posed using the same initial rotations across all poses $\{\hat{c}_1^i, \hat{c}_2^i, \hat{c}_3^i \}$. Here $c_1^i$ stands for the base of the second phalange, $c_2^i$ stands for the base of the third phalange, and $c_3^i$ the end of the third phalange.

Denote the initial rotations of the first, second and third phalanges as $I_1$, $I_2$ and $I_3$. We parametrize initial rotations of a finger by 5 values $\alpha = \{\alpha_1, ..., \alpha_5\}$, such that $I_1 = R_z(\alpha_3) R_y(\alpha_2) R_x(\alpha_1).$ \Anastasia{it is in reversed order to much the way the rotations are applied in Htrack, probably this should not  go to the paper} $I_2 = R_z(\alpha_4)$ and $I_3 = R_z(\alpha_5)$, where $R_x$, $R_y$ and $R_z$ are rotations around axis $X$ (side), $Y$ (up) and $Z$(front). We choose not to allow initial rotations around $X$ and $Y$ for the second and third segment partially to decrease the smallest number of required hand poses, partially because it seems that this rotations do not exist in real hand.

Denote the rotations given by the values of the joint angles for the pose $i$ for the phalanges as $J_1^i$, $J_2^i$ and $J_3^i$.
\begin{equation*}
J_1^i = R_x(\theta_2^i) R_z(\theta_1^i),
J_2^i = R_x(\theta_3^i),
J_3^i =  R_x(\theta_4^i),
\end{equation*}
where $\theta_1^i$ and $\theta_2^i$ are abduction and flexion of the first phalange and $\theta_3^i$ and $\theta_4^i$ are flexions of the second and third phalanges at the pose $i$. 
The positions of the centers posed with the same initial transformations are computed as following:
\begin{align*}
& \hat{c}_1^i =  t_1 + I_1   J_1^i    l_1  u, \\
& \hat{c}_2^i = t_1 + I_1    J_1^i   (t_2 + I_2  J_2^i  l_2  u), \\
&\hat{c}_3^i =  t_1 + I_1    J_1^i   (t2 +  I_2   J_2^i  (t_3 + I_3  J_3^i  l_3 u)),
\end{align*}
where $t_1$, $t_2$, $t_3$ and $l_1$, $l_2$, $l_3$ are the translations and length of the bases of the phalanges 1, 2 and 3. They are computed from the input locations of the centers are constants; $u$ is a vector along the axis $Y$ (up).
\end{DRAFT}
