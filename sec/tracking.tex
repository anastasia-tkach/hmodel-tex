\section{Tracking}

\input{fig/corresp/item.tex}
\paragraph{Element Correspondences}
\TODO{Anastasia, please add and describe \Figure{corresp}, that is how do we compute correspondences to a single convolution element}.

\paragraph{Element-Wise Correspondences}
Our correspondence search leverages the structure of \Eq{convsurf}, by decomposing the surface in several elementary convolution elements $\element$ (a decomposition of the shape in \todo{pill} and \todo{wedge} implicit primitives) and their associated implicit functions $\implicit_e$. Given a point $\point$ in space, the implicit function of the whole surface can be written by evaluating the expression:
\begin{equation}
\implicit_\surface(\point) = \argmin_{e=1 \dots E} \implicit_e(\point)
\label{eq:piecewise}
\end{equation}
Therefore, given a query point $\point$, we can first compute the closest-points $\footpoint_e = \proj_{\element}(\point)$ to each element independently; within this set, the closest-point projection to the full model $\footpoint = \proj_\surface(\point)$ is the one with the smallest associated implicit function value $\implicit_e(\point)$.
\TODO{should we mention how long this takes?}

\paragraph{Frontal Correspondences}
In monocular acquisition, an oracle registration algorithm aligns the portion of the model that is \emph{visible} from sensor viewpoint to the available data. Hence, when computing ICP's closest-point correspondences, only the portion of the model currently visible by the camera should be considered~\cite{tagliasacchi2015robust}. Given the camera direction $\camdir$, we can test whether the retrieved footpoint $\footpoint$ is back-facing by testing the sign of $\camdir \cdot \mathcal{N}_\surface(\footpoint)$, where the second term is the object's normal at $\footpoint$. As illustrated in 2D in \Figure{visibility}, whenever this test fails, there are additional candidates for closest point that must be checked: (1)~the closest-point on the silhouette of the model (e.g. $\point_{2,4,5,7}$), and (2)~the front facing planar portions of convolution elements (e.g. $\point_{3}$). These additional correspondences for the query point are computed, and the one closest to $\point$ becomes our front-facing footpoint $\footpoint$.
\input{fig/visibility/item.tex}


\input{fig/silhouette/item.tex}
\paragraph{Silhouette Computation}
The \emph{silhouette} is a (3D) curve separating front-facing from back-facing portions of a shape. \Anastasia{Normally by silhouette people mean the outline of the shape and everything inside of the outline. "A silhouette is the image of a person, animal, object or scene represented as a solid shape of a single color, usually black, its edges matching the outline of the subject (Wikipedia)". Also, silhouette is always in 2D, like in our silhouette energy} In our visibility-aware search, many points will be mapped to this curve. Similarly to \cite{tagliasacchi2015robust}, to compute the silhouette we approximate the perspective camera of the sensor with an orthographic one. Under this assumption finding the (3D) silhouette $\partial \surface$ becomes much simpler. 

\Anastasia{Updated}
We first shift all the model centers to have zero coordinate at the axis Z (which coincides with camera axis). We compute the cross-section of the model with XY plane. This cross-section consists of  circles and line segments (see \Figure{silhouette}, left). Now we need to find a set of circle segments and line segments that are on the outline of the cross-section. This is done by computing intersection and tangency points of every circle with every other circle and every line segment. The resulting structure can be thought of as a graph with intersection and tangency points as vertices and circle and line segments as edges. We traverse this graph starting from the upper left or any other point that is guaranteed to be on the outline. From every vertex we follow the edge with the next polar angle from the one that we came from (for the circle the polar angle is computed for a tangent at that point). This way  we always stay on the outline and never go inside of the cross-section.

Note that if a finger is in front of the palm, we still want to have its outline for the benefit of the correspondences that are back-facing to the finger.
Therefore we separately compute the outline for the palm and for the fingers and merge them together afterwards. The merging is done by removing the part of the finger outline that is inside of the palm outline and part of the palm outline that is inside of the finger outline. The fingers outline only modified if it is the outline of the base finger segment, because it is OK for the finger tip outline to be inside of the palm outline.

Once the 2D silhouette has been computed, it can be re-projected in 3D; see \Figure{silhouette}.
\Anastasia{End of update}

\AT{do you have to pass the silhouette back to CUDA to compute correspondences?}
\Anastasia{the outline is computed on CPU, the outline projections are computed on GPU. My function for computing  correspondences looks exactly like Algorithm 1 that I have wrote.}

\FINISH
