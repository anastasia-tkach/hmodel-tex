\section{Tracking}
\begin{DRAFT}
High level description of these three components:
(1) We need to modify correspondences to make them compatible to single vantage point
(2) we need to render the convolution surface to compute 2D alignment energies
\end{DRAFT}

\subsection{Correspondences}
% \paragraph{Frontal Correspondences}
In monocular acquisition, an oracle registration algorithm aligns the portion of the model that is \emph{visible} from sensor viewpoint to the available data. Hence, when computing ICP's closest-point correspondences, only the portion of the model currently visible by the camera should be considered~\cite{tagliasacchi2015robust}. Given the camera direction $\camdir$, we can test whether the retrieved footpoint $\footpoint$ is back-facing by testing the sign of $\camdir \cdot \mathcal{N}_\surface(\footpoint)$, where the second term is the object's normal at $\footpoint$. As illustrated in 2D in \Figure{visibility}, whenever this test fails, there are additional candidates for closest point that must be checked: (1)~the closest-point on the silhouette of the model (e.g. $\point_{2,4,5,7}$), and (2)~the front facing planar portions of convolution elements (e.g. $\point_{3}$). These additional correspondences for the query point are computed, and the one closest to $\point$ becomes our front-facing footpoint $\footpoint$. The additional computational cost caused by front-facing correspondences with an average of \todo{1000 points/frame} is \todo{100 $\mu s$/iteration}.
\input{fig/visibility/item.tex}

\input{fig/silhouette/item.tex}
\paragraph{Silhouette Computation}
The \emph{object-space silhouette} $\partial \surface$ is a (3D) curve separating front-facing from back-facing portions of a shape~\cite[Sec.1]{olson2006eg}. Similarly to \cite{tagliasacchi2015robust}, to simplify the silhouette computation we approximate the perspective camera of the sensor with an orthographic one. We then offset all convolution elements on the 2D camera plane, and perform a cross-section with this plane. Spheres are replaced with circles and planes/cylinders with segments; see~\Figure{silhouette}-(left). We then compute an \emph{arrangement}, splitting curves whenever intersection or tangency occurs~\Figure{silhouette}-(center). We traverse this graph, starting from a point that is guaranteed to be on the outline (e.g. a point on the bounding box). The traversal selects the next element as the one whose tangent forms the smallest counter-clockwise angle thus identifying the silhouette. Once the 2D silhouette has been computed, it can be re-projected in 3D; see \Figure{silhouette}. Note the process described above would compute the \emph{image-space silhouette} of our model. Therefore, we apply the process above to palm and fingers separately, and merge them in a second phase. The merge process simply checks whether vertices $v \in \partial \surface$ are contained within the model, that is it discards those where $\implicit_\surface(v)<0$. In our experiments, the average computation of silhouette on the CPU takes \todo{\emph{100 $\mu$s/iteration}}.
 
\subsection{Personalizing the PCA model.}
\TODO{Anastasia: quickly explain 5-10 lines? MSR?}
\\
\begin{DRAFT}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam.
\end{DRAFT}
 

\endinput

% \AT{I am very confused about what you wrote below, it doesn't look like an optimization problem at all... so in ``Chain Optimization'' I simply formalized what I had hand-written in the past -- please let me know if everything is clear!!.}
%
% \begin{DRAFT}
% To compute initial rotations we formulate an optimization problem for all the input hand poses. Denote the set of model centers in pose $i$ as $C_i = \{c_1^i, ... c_{|C_i| }^i\}$. As a preprocessing step we compute a rigid rotation between the rigid palm centers (the centers that are not articulated or elastic) and rotate all the poses to the same position, such that base centers of their fingers match together.
%
% The initial rotation is computed separately for each finger, so let us just discuss one finger. We minimize the norm of the difference between the centers of the finger at the current $i$ pose $\{c_1^i, c_2^i, c_3^i \}$ and the positions of the centers posed using the same initial rotations across all poses $\{\hat{c}_1^i, \hat{c}_2^i, \hat{c}_3^i \}$. Here $c_1^i$ stands for the base of the second phalange, $c_2^i$ stands for the base of the third phalange, and $c_3^i$ the end of the third phalange.
%
% Denote the initial rotations of the first, second and third phalanges as $I_1$, $I_2$ and $I_3$. We parametrize initial rotations of a finger by 5 values $\alpha = \{\alpha_1, ..., \alpha_5\}$, such that $I_1 = R_z(\alpha_3) R_y(\alpha_2) R_x(\alpha_1).$ \Anastasia{it is in reversed order to much the way the rotations are applied in Htrack, probably this should not  go to the paper} $I_2 = R_z(\alpha_4)$ and $I_3 = R_z(\alpha_5)$, where $R_x$, $R_y$ and $R_z$ are rotations around axis $X$ (side), $Y$ (up) and $Z$(front). We choose not to allow initial rotations around $X$ and $Y$ for the second and third segment partially to decrease the smallest number of required hand poses, partially because it seems that this rotations do not exist in real hand.
%
% Denote the rotations given by the values of the joint angles for the pose $i$ for the phalanges as $J_1^i$, $J_2^i$ and $J_3^i$.
% \begin{equation*}
% J_1^i = R_x(\theta_2^i) R_z(\theta_1^i),
% J_2^i = R_x(\theta_3^i),
% J_3^i =  R_x(\theta_4^i),
% \end{equation*}
% where $\theta_1^i$ and $\theta_2^i$ are abduction and flexion of the first phalange and $\theta_3^i$ and $\theta_4^i$ are flexions of the second and third phalanges at the pose $i$.
% The positions of the centers posed with the same initial transformations are computed as following:
% \begin{align*}
% & \hat{c}_1^i =  t_1 + I_1   J_1^i    l_1  u, \\
% & \hat{c}_2^i = t_1 + I_1    J_1^i   (t_2 + I_2  J_2^i  l_2  u), \\
% &\hat{c}_3^i =  t_1 + I_1    J_1^i   (t2 +  I_2   J_2^i  (t_3 + I_3  J_3^i  l_3 u)),
% \end{align*}
% where $t_1$, $t_2$, $t_3$ and $l_1$, $l_2$, $l_3$ are the translations and length of the bases of the phalanges 1, 2 and 3. They are computed from the input locations of the centers are constants; $u$ is a vector along the axis $Y$ (up).
% \end{DRAFT}

% The overwhelming majority of hand  tracking systems uses joint angles as an optimization variable. Given the vector of joint angles $\theta$, one also needs to know the initial rotations of the fingers with respect to the palm to be able to pose the model. The precise tracking is impossible without having the correct initial rotations, because the joints at the base of fingers cannot rotate around their axis, thus initial rotations constrain the possible poses that a hand model can assume.
% The initial rotations of the fingers can be guessed approximately because the are close to identity, but with the thumb it is much more difficult. \Anastasia{To prove this, I could alter the rotations in our system and show that tracking gets worse.}