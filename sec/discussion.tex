% !TEX root = ../hmodel.tex
\section{Discussion}
\label{sec:discussion}
\input{fig/motiontypes/item.tex}

% \paragraph{Calibration}
% \Anastasia{The calibration is still not perfect, there is several components to it, centers position, initial transformations, joint limits and PCA (initial transformations, joint limits and PCA seem to be similar for all humans, but in a perfect world we could try to optimize for all of them).}
% \Anastasia{Maybe, this goes too far, but we might say that our model calibration has so little degrees of freedom compared to a triangles mesh, that it does not depend on a prior as heavily as triangular mesh-based systems. Those systems cannot work without a prior - too many degrees of freedom. And a prior means acquiring a database of hands. Not any research team can accomplish that.}
% \AT{Perhaps, we should compare volumetric v.s. surface representations here? How triangle meshes need substantial regularization, the specification of control skeleton, skinning, collision elements, etc... while our models achieve very good visual fidelity by just positions and radii. Mark, if you can see a nice way of saying this, I'd really appreciate it.}

Our analysis demonstrates how convolution tracking templates are particularly well suited for real-time hand-tracking. Our calibration and tracking algorithms are simple to implement, efficient to optimize for, and allow for the geometry to be represented with high fidelity. 
While the calibration algorithm is currently implemented in Matlab, we are confident real-time performance can be achieved with a simple C++ port of our code. 
% 
The recent system from Taylor and colleagues~\cite{taylor2016concerto}, that will appear at {SIGGRAPH'2016}, has also demonstrated excellent tracking quality. Their formulation employs a triangular mesh model and optimizes it in a semi-continuous fashion. However, as their model is articulated through linear blend skinning, they suffer of visible joint collapse artifacts. Conversely, our model is volumetric and naturally overcomes this shortcoming; see \VideoNoJointCollapse{}.
% 
Although it is difficult to predict whether surface or volumetric models will eventually prevail, we believe the simplicity of our representation will lead to extremely performant articulated tracking algorithms.

\paragraph{Generative tracker}
% \Anastasia{We can stress again that we do not have re-initialization not because it is somehow fundamentally impossible in our method, but because we want to demonstrate how good our tracking is. It basically only brakes in limitation cases (fast motion, other objects, rotating thumb, out of sensor range.) For that we could totally add some powerful re-initialization to make the system even more robust.} \Anastasia{Also, I think we could mention that our system can be optimized to 120 FPS maybe? The tracking quality degrades with speed of motion, that is with number of frames per some ``quantity of motion''. With faster frame rate the system will definitely get better.} \AT{not fully true, faster framerate means more noise!}
We would like to stress that our real-time tracking algorithm is \emph{purely} generative, yet in this paper we demonstrated this yielding unprecedented levels of robustness to tracking failure. Currently the only sequences that incur in loss-of-tracking are the ones reported in our limitations \VideoLimitations{}. We believe this is due to the ability to optimize at a constant 60Hz rate, and to the quality of the calibrated model. Discriminative algorithms could still be necessary to compensate for situations where the hand re-appears from complete occlusions, but we believe their role for real-time tracking will diminish as RGBD sensors will start offering 120Hz imaging.  

\paragraph{Downsampling}
% \Anastasia{Also, I think it is important to mention that in all the sequences the depth data is downsampled 4 times. Otherwise, the reviewers might say that our tracking looks better because the hand is very close to the sensor. It is close than that it can be seen better}
Although the \realsense{} sensor is a short range camera, in this work we have downsampled the depth image 4 times to \todo{QVGA} format with a median filter, giving an average of \todo{10000} pixels/frame for optimization; which is approximatively the number of samples found on a hand in long-range cameras. The recent work of \cite{taylor2016concerto} reports a total of 192 pixels/frame, therefore enabling multi-CPU performance without loss of tracking precision. Inspired by this work, we have experimented with further downsampling and reached analogous conclusions. However, the computational bottleneck of the \emph{htrack} system we extend lies in the overhead caused by render/compute context switching. While this is currently an issue, we would like to note that this problem is likely to be resolved with the adoption of \emph{compute shaders}~(OpenGL 4.3).

\paragraph{Reproducibility}
% 
The weights of energy terms used in tracking and calibration optimizations have been identified by manually tweaking the runtime until our tracker reached the desired performance level. 
The parameters of our system are $\tau_{d2m}=1$, $\tau_{m2d}=.5$, $\tau_{rigid}=.3$, $\tau_{valid}=1e2$, $\tau_{pose}=1e4$, $\tau_{limits}=1e7$ and $\tau_{collision}=1e3$. We use 7 iterations for the tracking LM optimization, while $lsqnonlin$ automatically terminates in 5-15 iterations. Further, as we adhere to the \textsc{lgpl} requirements of \emph{htrack}, we (upon acceptance) release our source code with parameters, datasets, as well as the specification of the energy gradients required for optimization. This will ensure complete reproducibility of our work.