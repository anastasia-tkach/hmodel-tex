\section{Related Works}
\AT{\textbf{WARNING}: copy/pasted from msr siggraph paper. Mark if you want you can re-write it by paraphrasing it?}
\TODO{it is missing a section on convolution models and implicit skinning}

\begin{DRAFT}
Classical approaches to hand tracking include wearing data gloves [Dipietro et al. 2008], markers [Wang and Popovic 2009; Zhao et al. 2012] or wearable cameras [Kim et al. 2012], but this type of user instrumentation can act as a barrier for unencumbered natural interaction. Therefore the field has moved towards vision-based techniques without direct user instrumentation. Techniques include discriminative approaches [Keskin et al. 2012; Xu and Cheng 2013; Sun et al. 2015], which directly estimate hand pose by extracting image features e.g. using classification and/or regression techniques. These often do not rely on temporal information and attempt to directly estimate the hand joint configuration, thus making them useful as robust re-initializers [Shotton et al. 2011]. Conversely, generative (or model-based) methods use an explicit hand model to recover pose across a temporal sequence of images [Oikonomidis et al. 2011; Tagliasacchi et al. 2015]. Discriminative approaches are typically robust, but lack the accuracy inherent in model fitting. Generative approaches can suffer from a dependency on needing to initialize from the previous frame, making recovery from errors more challenging. Hybrid methods address this by combining discriminative and generative to improve the robustness of frame-to-frame model fitting with per-frame reinitialization [Ballan et al. 2012; Sridhar et al. 2013; Sridhar et al. 2015; Qian et al. 2014; Sharp et al. 2015].

\paragraph{Single RGB-based Approaches}:
Early vision-based systems used monocular RGB cameras, making the problem very challenging (see [Erol et al. 2007] for a survey). Understandably most approaches were offline. Discriminative approaches used small databases of restricted hand poses [Athitsos and Sclaroff 2003; Wu and Huang 2000] and had limited accuracy. Generative methods used models with restricted degrees of freedom (DoF), working with simplified hand representations based on 2D, 2.5D or inverse kinematics (IK) (e.g. [Stenger et al. 2001; Wu et al. 2001]), again resulting in limited accuracy. Bray et al. [2004] used a more detailed 3D hand model. Further user-personalization was added by de La Gorce et al. [2011], who automatically applied a scaling to the bones in the hand model during tracking. An early example of online (10Hz) tracking with a simplified deformable hand model is [Heap and Hogg 1996]. This work, as with other RGB-based methods, struggled with complex poses, changing backgrounds, and occlusions, thus limiting general applicability. In our work, we use a single commodity depth camera; while such cameras typically also provide RGB data, we focus purely on the depth signal to ensure robustness to lighting changes and to reduce computational load. Multi-camera Performance Capture: Possibly the first attempt at unencumbered hand tracking was by Rehg and Kanade [1994], using two video cameras to resolve 27 DoF using a Gauss-Newton solver and local edge search based on a capsule model. Their system ran at 10Hz, with the limitations that the hand should be almost entirely unoccluded. There has also been recent work on high quality, offline (non-interactive) performance capture of hands using multi-camera rigs. Ballan et al. [2012] demonstrate high-quality results closely fitting a detailed scanned mesh model to complex two handed and hand-object interactions. Zhao et al. [2012] use a depth camera, motion capture rig, and markers worn on the user’s hand to capture complex single-hand poses, again offline. Wang et al. [2013] and Tzionas et al. [2015] show complex hand-object interactions by using a physics engine, but take minutes per frame. The first multi camera real-time system was that of Sridhar et al.[2013], where five RGB cameras and a time-of-flight (ToF) sensor is used to track a user’s hand at 10Hz. We provide a direct comparison later in this paper. The above systems can produce highly accurate results, but are impractical for interactive consumer scenarios due to their computational cost and the heavyweight multi-camera setups. Single Depth Cameras: The advent of consumer depth cameras such as Kinect has made computer vision more tractable, for ex ample through robustness to lighting changes and improved invariance to foreground and background appearance. However, most high-accuracy real-time methods are still extremely computationally demanding (typically using the GPU), making their use on mobile devices still limited.

\paragraph{Generative Approaches} 
Oikonomidis et al. [2011] present a gener ative method based on particle swarm optimization (PSO) for full DoF hand tracking (at 15Hz) using a depth sensor. The hand is tracked from a known initial pose, and the method cannot recover from loss of track. Other stochastic optimization schemes have also been explored for real-time hand model fitting but again these lack the benefits of discriminative approaches [Oikonomidis et al. 2011; Makris et al. 2015]. Melax et al. [2013] use a generative approach driven by a physics solver to generate 3D pose estimates. Both Fleishman et al. [2015] and Tagliasacchi et al. [2015] show impressive high speed tracking results using a fast, articulated variant of ICP. These systems however do not recover from tracking loss, and the latter requires the user to wear a wristband for hand localization. Despite this, Tagliasacchi et al. [2015] have shown state of the art results for hand tracking. Discriminative Approaches: Keskin et al. [2012] propose a discriminative method using a multi-layered random-forest to predict hand parts and thereby to fit a simple skeleton. The system runs at 30Hz on consumer CPU hardware, but can fail under occlusion. A variety of systems extend this work [Tang et al. 2013; Tejani et al. 2014; Tang et al. 2015; Sun et al. 2015] using novel classification or regression architectures to demonstrate more complex poses. Other discriminative methods include [Xu and Cheng 2013; Oberweger et al. 2015b; Li et al. 2015; Neverova et al. 2015; Poier et al. 2015], which estimate hand pose directly in a single shot using different cascaded learning-based architectures. Wang et al. [2009; 2011] demonstrate a discriminative nearest-neighbor lookup scheme using a large hand pose database, and IK for pose refinement. Nearest neighbor methods are highly dependent on the database of poses, and can struggle to generalize to unseen poses. Purely discriminative systems do not include an explicit model-fitting step, and so results may often not be kinematically valid (e.g. implausible articulations or finger lengths).

\paragraph{Hybrid Approaches} 
Qian et al. [2014] extend [Oikonomidis et al. 2011] by adding a guided PSO step and a reinitializer that requires fingertips to be clearly visible. Sridhar et al. [2014] use a sum-of- Gaussians representation for efficient model fitting with reinitialization. All these systems so far are based on simple hand models; our approach exploits a full 3D hand mesh model that, as shown, is better able to fit to the observed data. Tompson et al. [2014] demon strate impressive hand tracking results using deep neural networks to predict feature locations and IK to infer a mesh skeleton. While real-time, the approach only tackles close-range scenarios. Sharp et al. [2015] use a detailed mesh model and PSO in combination with a robust fern and forest re-initializer to generate impressive results, including far distances or moving camera scenarios, but with heavy demand of a high-end GPU.

\paragraph{Commercial systems} 
Beyond this research, there have also been commercial hand tracking systems. The 3Gear/NimbleVR Systems [3Gear Systems Inc 2013] (based on prior work ofWang et al. [2009; 2011]) and the second-generation software for the Leap Motion [Leap Motion Inc 2013] have shown tracking of a range of complex poses, including two-handed interaction. Whilst their videos are extremely impressive, Sharp et al. [2015] have shown that these systems suffer from poor tracking of complex poses. In our work we directly compare to, and compare favorably to, Sharp et al..
\end{DRAFT}

%----------------------------------------------------------------------
%----------------------------------------------------------------------
%----------------------------------------------------------------------
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\endinput

% MORE STUFF
% http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning/

\TODO{\cite{bogo2015detailed}}

%--- ON SPHERE MODELS
\TODO{A higher precision can be obtained by increasing the number of primitives, which defeats the purpose of model simplicity. Convolution surfaces representation gives higher precision for the same number of building blocks. \textcolor{mygray}{Add experimental or theoretical support for convolution surfaces.}}

%--- LINEAR BLEND SKINNING
\TODO{The Linear Blend Skinning approach used to pose the triangular mesh model in previous works (\cite{sharp2015accurate}, \cite{schroder2013analysis} ) creates artifacts, the fingers looks like made from rubber. The spheres/cylinders model is not suitable for realistic animation, therefore a re-targeting step to a template mesh is required. Retargeting does not only demand additional effort, but also brings additional imprecision. }

%--- IMPLICIT SKINNING
\TODO{The state of the art approaches in hand skinning are implicit surfaces-based (\cite{vaillant2013implicit},  \cite{vaillant2014robust} ).  A convolution surfaces model serves a ready to use input for such an approach.}

%--- CLOSEST POINT COMPUTATIONS
\TODO{For model based tracking the main operation is to find the closest point on the model for a given data point. This operation is can be done in closed for each rigid segment with spheres/cylinder and convolution surfaces model representation. For a triangular mesh this operation has complexity linear in number of triangles. Thus, it is necessary to simplifying assumptions and/or more complex optimization to allow the hand tracking system to run in real time. (Look how different systems deal with this problem). Moreover, the triangular mesh has (much) more degrees of freedom than the underlying problem. Without additional regularization, rigid parts of the hand model can deform to fit the data and the individual vertices can shift to fit the sensor noise.}

%--- JUST A LIST OF PAPERS?
\todo{A number of hand tracking algorithms has been recently proposed  Keskin et. al. \cite{keskin2012hand}, Melax et. al. \cite{melax2013dynamics}, Tang et. al. \cite{tang2013real}, Oikonomidis et. al. \cite{oikonomidis2014evolutionary}, Schroder et. al. \cite{schroder2014real},
Tompson et. al. \cite{tompson2014real}, Qian et. al. \cite{qian2014realtime},  Tagliasacchi et. al. \cite{tagliasacchi2015robust}, Sridhar et. al. \cite{sridhar2015fast}, Sun et. al. \cite{sun2015cascaded} and Sharp et. al. \cite{sharp2015accurate}.}

\begin{itemize}

\item Albrecht et al. \cite{albrecht2003construction} developed an approach for creating an anatomically realistic hand model that includes bones and muscles structure. Their approach requires several prerequisites including plaster cast of a human hand and laser scanner for manually creating a physically realistic hand template. Given user-defined correspondences between 3D feature points and the hand image, a specific hand model is created by deforming a generic hand model. 

\item Rhee et. al. \cite{rhee2006human} use a single image of a hand at rest pose to infer joint hand joint locations from skin creases. Given the skeleton obtained at the previous step and the hand contour from the image, they deform a template hand mesh to fit this data. 

\item Straka et al. \cite{straka2012simultaneous} also fit the template mesh with attached skeleton to 3D data. The model is deformed to explain the data while keeping the vertices attached to their corresponding bones. It is on clear whether the approach whether be able to handle a hand motion sequence, since the results are demonstrated on a full body model.

\item Taylor et. al. \cite{taylor2014user} generate a user-specific hand model from an RGBD video sequence. The model is represented as a triangular mesh with an embedded skeleton. In each frame the hand pose is initialized using an appearance-based tracking algorithm. The hand model parameters are found by solving a single optimization problem formulated for the entire video sequence which also finds hand pose in each frame. 

\item Khamis et. al.  \cite{khamis12learning} fit a hand model for a specific user by finding its shape coordinates in the basis of mesh matrices and bones locations. As in the approach by Taylor et. al, they optimize simultaneously for pose and shape parameters in all the frames of an RGBD sequence across all the subjects. Requires large number of subjects as a regularization for excessive degrees of freedom. 

The results generated by the approaches listed above could be used as an input for our system to create a hand model representation adapted for efficient tracking and animation.


\end{itemize}