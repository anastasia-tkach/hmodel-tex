% !TEX root = ../hmodel.tex
\section{Results}
\label{sec:results}


\todo{In our research, beyond releasing a dataset specifically designed to evaluate high precision tracking and propose metrics to quantitatively evaluate  performance in an algorithm-independent fashion.}

\subsection{Database}
\begin{DRAFT}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{DRAFT}
\todo{seq1: concerto1 - rigid motion, clenching fist, seq2: shidhar1 - extending one finger, seq3: shidhar2 - fingers contact, seq4: shidhar4 - crossing fingers, seq5: shidhar5 - pinching, seq6: sharp1 - fast articulated motion, unlikely poses, extending finger, seq7: sharp2 - fast rigid, seq8: sharp3 - rotating fist}

\providecommand{\depthrend}{\mathcal{R}}
\providecommand{\metricone}{E_\text{3D}}
\providecommand{\metrictwo}{E_\text{2D}}
\subsection{Comparison Metrics}
Taylor and colleagues \shortcite{taylor2016concerto} have reported how recent hand tracking algorithms \cite{sharp2015accurate} \cite{tagliasacchi2015robust} have reached human precision in determining the location of key features (e.g. fingertips and wrist position). Therefore, publicly available datasets like \cite{tompson2014real} and \cite{sridhar2013multicam} \MP{add: ``based on human labeling'' to make it clear, but is this true for these methods?} have become less suitable for quantitatively evaluating the quality of high-precision tracking. 
% 
For this reason, we propose several easy-to-compute metrics to evaluate the quality of generative algorithms for modeling/tracking. A core element that makes these metrics appealing is that, much like key feature positions, they are completely algorithm (and hence model) independent. This is essential, as it will enable the research community to validate and compare results through quantitative analysis. 
% 
We achieve this goal by expressing these metrics exclusively as a function of the acquired depth image $\depth_n$, and of the depth image $\depthrend_{n}$ of the rendered model. Note that below we drop the subscript $n$ for notational brevity, and that $\point \in \depth$ only considers points within the RoI. \AT{We have to explain why the ``golden energy'' is not sufficient; perhaps we can say more once we see the results.}


\input{fig/motiontypes/item.tex}

\paragraph{Fitting residuals (3D)}
As we are seeking high-quality alignement every pixel from the sensor data should lie in the proximity of the model. Here the $\proj_\depthrend$ computes the closest point correspondence to the point cloud of the rendered model:
\begin{equation}
\metricone = \frac{1}{|\depth|}\sum_{\point \in \depth} \| \point - \proj_\depthrend(\point) \|_2^2
\label{eq:metric1}
\end{equation}
Differently from tracking these can be fetched with a kd-tree.

\paragraph{Silhouette residuals (2D)}
Computing model-to-data registration residuals in 3D is not meaningful \todo{because blah}. Therefore, we evaluate the registration residual in the 2D projective space. 
\begin{eqnarray}
\metrictwo 
&=& \frac{1}{|\depthrend \setminus \partial\depth|} 
\sum_{\pixel \in \depthrend \setminus \partial\depth} \| \pixel - \proj_{\partial\depth}(\pixel) \|_2 \\ 
&=& \frac{1}{|\depthrend \setminus \partial\depth|} 
\sum_{\pixel \in \depthrend \setminus \partial\depth} \text{DT}(\pixel)
\label{eq:metric2}
\end{eqnarray}
Note how above each term above can be evaluated efficiently by pre-computing the 2D euclidean distance transform DT of the RoI's (image-space) silhouette $\partial \depth$. Further, note that DT evaluates to zero for a pixel inside the silhouette. 



\paragraph{Golden Energy}
Another metric we consider is the golden energy from Sharp and colleagues~\shortcite{sharp2015accurate} that was employed to drive their PSO real-time tracker:
\begin{equation}
E_{\text{Au}} = \sum_{ij} \rho(\depth(i,j) - \depthrend(i,j))
\label{eq:golden}
\end{equation}
where $\rho(e)=\min(|e|,\tau)$ is a truncated $\ell_1$ distance to make the metric less sensitive to outliers ($\tau=100mm$). Pixels outside the region of interest are mapped to the far plane $\depth^\text{max}$.

\input{fig/barchart/item.tex}
\paragraph{Comparison to HTrack}
% \protect\cite{tagliasacchi2015robust}


