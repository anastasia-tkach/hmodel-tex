\section{Calibration}
In order to tailor hand model for a specific user, we acquire a set of $N$ 3D measurements $\depth_1 \dots \depth_N$ of the user's hand in different poses. \todo{Such data can come from a static acquisition setup or not.} \todo{$\centers$ is a matrix stacking all $\ballcenter_i$ while $\radii$ is a vector stacking all $r_i$.}
% 
\todo{Let us split the parameters of our model in two sets, the first controlling the pose $\parpose$, while the second set controls the posture $\parposture$.}
% 
\todo{attempt to fit a single convolution model through this data.}
% 
\begin{eqnarray}
\argmin_{\parposture, \centers, \radii}
\sum_{n=1}^N 
\sum_{\mathcal{T} \in \mathcal{T}_*} 
w_\mathcal{T} E_\mathcal{T}(\depth_n, \parpose_n, \parposture, \centers, \radii)
\end{eqnarray}
% 
\todo{Our terms $\mathcal{T}$ are...}
\begin{description}[labelsep=0em,labelwidth=.4in,labelindent=1cm]
\item[d2m] each data point should be close to our model
\item[m2d] the model should lie close to the data
\item[rigid] elements are posed rigidly
\item[valid] elements should not degenerate
\end{description}
% 
Our calibration optimization splits this problem in two alternating optimization steps:
% 
\begin{eqnarray}
\argmin_{\parposes, \parposture} 
\sum_{n=1}^N 
\sum_{\mathcal{T} \in \mathcal{T}_*}
w_\mathcal{T} E_\mathcal{T}(\depth_n, \parpose_n, \parposture, \centers, \radii) 
&& \text{(Step 1)}
\\
\argmin_{\centers, \radii} 
\sum_{n=1}^N 
\sum_{\mathcal{T} \in \mathcal{T}_*}
w_\mathcal{T} E_\mathcal{T}(\depth_n, \parpose_n, \parposture, \centers, \radii)
&& \text{(Step 2)}
\end{eqnarray}

\paragraph{Initialization}
The initial values of the parameters for each pose  $\{[c^0_1 , r^0]$, ..., $[c^0_n, r^0]\}$ are found tracking an input depth sequence with a template hand model globally scaled such that the length of the model hand matches the data.




% The first reason is that we want ensure that the resulting hand model will be able to assume all the poses that a real hand can, or at least the subset of poses for which we have the point clouds.
% The second reason is that the artifacts present in some input point clouds might be compensated by the others, so it is possible to accumulate information this way.
% For fitting we use the same optimization framework as for tracking (in a hope to extend it to fitting \& tracking real time system).
% The main difference is that instead of the joint angles $\theta$, the hand model is parametrized by convolution surface centers locations and radii $[c , r]$.
% \begin{equation*}
% \sigma =\underset{\sigma}{\operatorname{argmin}} \; E_\text{d2m} + E_\text{m2d} + E_\text{rigid} + E_\text{valid}
% \end{equation*}
% \subsection{Optimization}

\paragraph{Data $\rightarrow$ Model Energy}
\todo{The data-model energy is computed in exactly the same way as in Htrack, except that the gradients are much more complicated.}
\begin{equation*}
E_{d2m} = \underset{p\in P}\sum \| p - q(\sigma)\|_2
\end{equation*}

\paragraph{Model $\rightarrow$ Data Energy}
\todo{Same as in HTrack, while when using photoscan we can sample the model and compute kdtree correspondences}
% The first attempt was to make the model-data energy also the same as in Htrack. But probably due to more non-linear gradients that expression was giving unstable optimization, because it was involving a projection operator. So, the model-data energy is replaced by a similar energy, but in 3D space. \Anastasia{Maybe we could just say that it is exactly the same?}.
% The model-data correspondences are also computed by rendering the model and the data, identifying the model points that are outside of the data silhouette and finding the closest data points in 2D using a distance transform. Afterwards for each 2D correspondence pair $\{m_{2D}, p_{2D}$\}, the original 3D points $\{m, p\}$ are looked up. We minimize the distance between $m$ and $p$ in the direction orthogonal to the camera ray that goes through $p$.
% \input{fig/optimization/item}
% \begin{equation*}
% E_\text{model-data} = \underset{m\in M}\sum n^T(p - m(\sigma))
% \end{equation*}

\paragraph{Rigid}
The consistency energy requires that for every rigid part of the skeleton the distances between every pair of vertices are the same for all the poses. The consistency energy does not apply to elastic parts of hand model.
\Anastasia{To demonstrate rigid parts I could color-code them on the skeleton topology image. The rigid parts are palm, wrist and each finger segment}

\paragraph{Non degeneracy}
This energy ensures that pills do not degenerate into spheres and wedges do not degenerate into pills.
A pill becomes degenerate if one of the spheres is completely inside of another sphere. A wedge becomes degenerate if a sphere is completely inside of the tangent cone of two other spheres. The energy switches on if a pill or a wedge is within a threshold of becoming degenerate and pushes the optimization away.

% \paragraph{Synchronizing rotations energy}
% \AT{This is now obsolete, as we encode common rotation frames for nodes of the kinematic chain} This energy ensures that initial rotations are similar for all the poses. The common initial rotations are computed as described in the section ``Initial Rotations''. The energy minimizes the distance between the current centers and the centers locations with common initial rotations. This energy is very important, because it constraints fingers motion in meaningful way. In the absence of this energy each joint bends in arbitrary direction.

\newpage

\begin{DRAFT}
\subsection{Initializing the kinematic chain.}
While our convolution model is fully specified by sphere positions and radii, most tracking systems, including the one we build upon, parameterize the pose through the specification of a kinematic chain. Every node of the kinematic chain is associated with a frame of reference according to which local transformations are specified~\cite[Fig.2-(middle)]{tagliasacchi2015robust}. For example, transformations at the base of the index finger are parameterized so to produce a \todo{flexion} by performing as rotation around the local $x$ axis, and an abduction as a \todo{rotation} around the local $y$ axis. In most systems the orthogonal frame origin and rotation is manually specified offline by a 3D modeling artist, and is therefore not adapted to the user. 
% 
While rotation frames for most fingers are typically initialized with a rotation matrix, it is extremely difficult to identify an appropriate rotation frame for the thumb articulation, and as we show in the video at frame \todo{[00:00]} this can result in imprecise tracking. Therefore in our technique we automatically discover the structure of the kinematic chain from input data. 

\paragraph{Chain Optimization}
The coordinate frame translations are simply a subset of sphere centers of our convolution skeleton $\skeleton$. \Anastasia{it is true, but translations and centers are different kinds of entities (point and vector). Can we say instead that the translations can be computed from the centers?} Conversely, the orientation of rotation in the local coordinate frames are computed as the solution of an optimization problem.
\end{DRAFT}
