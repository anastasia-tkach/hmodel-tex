% !TEX root = ../hmodel.tex


\input{fig/posing/item.tex}
\section{Tracking}
\label{sec:tracking}
Given a calibrated hand model $\model$, in our real-time tracking algorithm we optimize the 26 degrees of freedom $\parpose$ so that our hand model matches the sensor input data; the generation of a calibrated model $\model$ for a user is detailed in \Section{modeling}. Directly extending the open source \emph{htrack} framework of \cite{tagliasacchi2015robust}, we write our tracking optimization in Gauss-Newton/Levenberg-Marquardt form
% 
\begin{eqnarray}
\parpose_t = \argmin_{\parpose}
\sum_{\mathcal{T} \in \termstrack} 
w_\mathcal{T} E_\mathcal{T}(\depth_t,\parpose,\parpose_{t-1})
\label{eq:htrack}
\end{eqnarray}
% 
where fitting energies are combined with a number of priors to regularize the solution and ensure the estimation of plausible poses. The energy terms $\termstrack$ in our optimization are:
% 
\begin{description}[labelsep=0em,labelwidth=.6in,labelindent=.25cm]
    \item[d2m] each data point is explained by the model
    \item[m2d] the model lies in the sensor visual-hull
    \item[pose] hand poses sample a low-dimensional manifold
    \item[kinematic] fingers respect joint limits and avoid collisions
    \item[temporal] the hand is moving smoothly in time
\end{description}
% 
We limit our discussion to the computational elements that need to be adapted to support our convolution model, while referring the reader to \cite{tagliasacchi2015robust} for further details. 
% 
Generally speaking, the similarity of two geometric models is measured by the symmetric Hausdorff distance $d_{X \leftrightarrow Y}$:
% 
\begin{eqnarray*}
d_{X \rightarrow Y} =& \max_{x \in X} \left[ \min_{y \in Y} d(x,y) \right] \\
d_{Y \rightarrow X} =& \max_{y \in Y} \left[ \min_{x \in X} d(x,y) \right] \\
d_{X \leftrightarrow Y} =& \max \{ d_{X \rightarrow Y}, d_{Y \rightarrow X} \}
\end{eqnarray*}
We therefore interpret our terms $E_{d2m}$ and $E_{m2d}$ as approximations to the asymmetric Hausdorff distances $d_{X \rightarrow Y}$ and $d_{Y \rightarrow X}$, where the \todo{costly to differentiate} ``$\max$'' operators are replaced by arithmetic means, and a robust $\ell_1$ distance is used for $d(x,y)$. 

\paragraph{Data $\rightarrow$ Model}
The first asymmetric distance minimizes the average closest point projection of each point $\point$ in the current data frame $\depth$:
%
\begin{equation}
E_{d2m} = \frac{1}{|\depth|}\sum_{\point \in \depth} \| \point - \proj_{\model(\pars)}(\point)\|_2^1
\label{eq:d2m}
\end{equation}
% 
Adapting this energy, as well as its derivatives, to the convolution models requires the specification of the projection operator $\proj_\model$ that is described in \Section{corresp}.

\paragraph{Model $\rightarrow$ Data}
The second asymmetric distance considers how our monocular acquisition system does not have a complete view of the model. While the 3D location is unknown, we can penalize the model from lying outside the sensor's image-space visual hull:
\begin{equation}
E_{m2d} = \frac{1}{|\model(\pars)|} \sum_{\pixel \in \model(\pars)} \| \pixel - \proj_{\depth}(\pixel)\|_2^1
\label{eq:m2d}
\end{equation}
In the equation above, the set of pixels $\pixel \in \model(\pars)$ is produced by the rasterization process detailed in \Section{rendering} that renders the model with the same intrinsic and extrinsic parameters of the sensor camera. 

\paragraph{Pose space retargeting}
\TODO{Should we say something here?}
\TODO{The wrist fitting term is missing and a convolution element for the wrist is used instead.}